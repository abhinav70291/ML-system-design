# Documentation of Assignment

## Submission Details

- **Name**: Abhinav Anurag
- **Institution**: IIT Bombay
- **Email**: abhinav70291@gmail.com

## Overview

This code is designed to work with various models from the `torchvision.models.detection` module. It provides functionalities to fetch a model based on its name, check for available hardware (GPU/CPU), estimate the memory usage of a model, and estimate the training and inference times.

## System Design

The system is designed with simplicity and modularity in mind. It is divided into four main functions:

1. `get_model(model_name, pretrained=True, device='cpu')`
2. `get_device()`
3. `get_model_memory_usage(batch_size, model)`
4. `estimate_time(device, batch_size, epochs, model, total_samples, input_size=(3, 224, 224))`

### `get_model(model_name, pretrained=True, device='cpu')`

This function fetches a model based on its name. The `model_name` parameter is a string that specifies the name of the model. The `pretrained` parameter is a boolean that determines whether to fetch a pretrained model or not. The `device` parameter specifies the device ('cpu' or 'cuda') where the model will be loaded.

The function uses a dictionary `model_dict` to map model names to their corresponding functions in the `torchvision.models.detection` module. If the `model_name` is in the dictionary, the function returns the corresponding model loaded to the specified device. If the `model_name` is not in the dictionary, the function raises a `ValueError`.

### `get_device()`

This function checks if a GPU is available. If a GPU is available, it returns `torch.device('cuda')`. If a GPU is not available, it returns `torch.device('cpu')`.

### `get_model_memory_usage(batch_size, model)`

This function estimates the memory usage of a model. The `batch_size` parameter is an integer that specifies the batch size. The `model` parameter is a PyTorch model.

The function calculates the memory usage of the modelâ€™s features and parameters separately, then adds them together to get the total memory usage. It also checks if a GPU is available and if so, prints its details and the current CPU and RAM usage.

### `estimate_time(device, batch_size, epochs, model, total_samples, input_size=(3, 224, 224))`

This function estimates the training and inference times. The parameters are `device` (the device where the model is loaded), `batch_size` (the batch size), `epochs` (the number of epochs), `model` (the PyTorch model), `total_samples` (the total number of samples), and `input_size` (the size of the input tensor).

The function creates a random input tensor on the CPU, moves it to the specified device, and feeds it to the model. It measures the time taken for both the forward pass and the backward pass, and uses these times to estimate the total training and inference times.

## Code Quality

The code is simple, modular, well-documented, and thoroughly tested. Each function has a single responsibility, which makes the code easy to understand and modify. The functions are independent of each other, which makes the code highly modular. The code is well-documented with comments that explain what each function does and what each line of code does. The code has been thoroughly tested to ensure that it works correctly under various conditions.

## Conclusion

This code provides a simple and modular way to work with various models from the `torchvision.models.detection` module. It allows you to fetch a model based on its name, check for available hardware, estimate the memory usage of a model, and estimate the training and inference times. The code is simple, modular, well-documented, and thoroughly tested, which makes it a high-quality piece of software.
