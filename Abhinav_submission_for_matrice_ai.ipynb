{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP2+AWDeF64ZFvhO0K7ZZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinav70291/ML-system-design/blob/main/Abhinav_submission_for_matrice_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEK6T8iWgG4M",
        "outputId": "d3f7206b-b812-44d2-bcfe-5683678b5fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100%|██████████| 170M/170M [00:03<00:00, 58.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: maskrcnn_resnet50_fpn\n",
            "Memory for Model features in GB is: 1.323264628648758 GB\n",
            "Memory for Model parameters in MB is: 169.38\n",
            "CPU usage: 39.70%\n",
            "RAM usage: 10.70%\n",
            "Memory usage (GB): 1.4886727072298527\n",
            "Estimated training time: 2142803.31 seconds\n",
            "Estimated inference time: 57.14 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: fasterrcnn_resnet50_fpn\n",
            "Memory for Model features in GB is: 1.2444044947624207 GB\n",
            "Memory for Model parameters in MB is: 159.28\n",
            "CPU usage: 65.00%\n",
            "RAM usage: 13.00%\n",
            "Memory usage (GB): 1.3999550566077232\n",
            "Estimated training time: 2102652.73 seconds\n",
            "Estimated inference time: 56.07 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCOS_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FCOS_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fcos_resnet50_fpn_coco-99b0c9b7.pth\" to /root/.cache/torch/hub/checkpoints/fcos_resnet50_fpn_coco-99b0c9b7.pth\n",
            "100%|██████████| 124M/124M [00:01<00:00, 79.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: fcos_resnet50_fpn\n",
            "Memory for Model features in GB is: 0.9617090225219727 GB\n",
            "Memory for Model parameters in MB is: 123.10\n",
            "CPU usage: 62.80%\n",
            "RAM usage: 12.00%\n",
            "Memory usage (GB): 1.0819226503372192\n",
            "Estimated training time: 1879516.21 seconds\n",
            "Estimated inference time: 50.12 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth\n",
            "100%|██████████| 130M/130M [00:01<00:00, 121MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: retinanet_resnet50_fpn\n",
            "Memory for Model features in GB is: 1.0137259662151337 GB\n",
            "Memory for Model parameters in MB is: 129.76\n",
            "CPU usage: 62.30%\n",
            "RAM usage: 12.70%\n",
            "Memory usage (GB): 1.1404417119920254\n",
            "Estimated training time: 2104735.31 seconds\n",
            "Estimated inference time: 56.13 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n",
            "100%|██████████| 136M/136M [00:03<00:00, 39.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: ssd300_vgg16\n",
            "Memory for Model features in GB is: 1.0622091889381409 GB\n",
            "Memory for Model parameters in MB is: 135.96\n",
            "CPU usage: 60.10%\n",
            "RAM usage: 12.90%\n",
            "Memory usage (GB): 1.1949853375554085\n",
            "Estimated training time: 445537.09 seconds\n",
            "Estimated inference time: 11.88 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\" to /root/.cache/torch/hub/checkpoints/ssdlite320_mobilenet_v3_large_coco-a79551df.pth\n",
            "100%|██████████| 13.4M/13.4M [00:00<00:00, 47.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: ssdlite320_mobilenet_v3_large\n",
            "Memory for Model features in GB is: 0.10252177715301514 GB\n",
            "Memory for Model parameters in MB is: 13.12\n",
            "CPU usage: 59.70%\n",
            "RAM usage: 12.80%\n",
            "Memory usage (GB): 0.11533699929714203\n",
            "Estimated training time: 31191.68 seconds\n",
            "Estimated inference time: 0.83 seconds\n",
            "_________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import psutil\n",
        "import time\n",
        "from torchvision.models.detection import *\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Here's a handy function to fetch a model based on its name\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def get_model(model_name, pretrained=True, device='cpu'):\n",
        "    model_dict = {\n",
        "        'maskrcnn_resnet50_fpn': maskrcnn_resnet50_fpn,\n",
        "        'fasterrcnn_resnet50_fpn': fasterrcnn_resnet50_fpn,\n",
        "        'fcos_resnet50_fpn': fcos_resnet50_fpn,\n",
        "        'retinanet_resnet50_fpn': retinanet_resnet50_fpn,\n",
        "        'ssd300_vgg16': ssd300_vgg16,\n",
        "        'ssdlite320_mobilenet_v3_large': ssdlite320_mobilenet_v3_large\n",
        "    }\n",
        "\n",
        "    if model_name in model_dict:\n",
        "        model = model_dict[model_name](pretrained=pretrained)\n",
        "        return model.to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# This function checks for a GPU, using a CPU as a backup\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Let's estimate how much memory a model might need\n",
        "# ----------------------------------------------------------------\n",
        "def get_model_memory_usage(batch_size, model):\n",
        "    float_bytes = 4.0  # Multiplication factor as all values we store would be float32.\n",
        "    features_mem = 0  # Initialize memory to store  features.\n",
        "\n",
        "    for param in model.parameters():\n",
        "        # here I am multiplying all shapes to get the total number per layer.\n",
        "        single_layer_mem = np.prod(param.shape)\n",
        "        single_layer_mem_float = single_layer_mem * float_bytes\n",
        "        single_layer_mem_MB = single_layer_mem_float / (1024 ** 2)  # Converting to MB\n",
        "\n",
        "        features_mem += single_layer_mem_MB  # Add to total feature memory count\n",
        "\n",
        "    # Calculate Parameter memory\n",
        "    trainable_wts = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable_wts = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "    parameter_mem_MB = ((trainable_wts + non_trainable_wts) * float_bytes) / (1024 ** 2)\n",
        "    print(\"Memory for Model features in GB is:\", features_mem * batch_size/1024, \"GB\")\n",
        "    print(\"Memory for Model parameters in MB is: %.2f\" % parameter_mem_MB)\n",
        "\n",
        "    total_memory_MB = (batch_size * features_mem) + parameter_mem_MB\n",
        "    total_memory_GB = total_memory_MB / 1024\n",
        "\n",
        "    # Checking if a GPU is available and if so, get its details\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(torch.cuda.memory_summary(device=device, abbreviated=False))\n",
        "        print(\"Device name:\", torch.cuda.get_device_name(device))\n",
        "        print(\"Memory Allocated:\", torch.cuda.memory_allocated(device)/1024**3, \"GB\")\n",
        "        print(\"Memory Cached:   \", torch.cuda.memory_reserved(device)/1024**3, \"GB\")\n",
        "\n",
        "\n",
        "    # Get CPU and RAM usage\n",
        "    cpu_usage = psutil.cpu_percent()\n",
        "    ram_usage = psutil.virtual_memory().percent\n",
        "    print(\"CPU usage: %.2f%%\" % cpu_usage)\n",
        "    print(\"RAM usage: %.2f%%\" % ram_usage)\n",
        "\n",
        "    return total_memory_GB\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Here's a way to estimate training and inference times\n",
        "# ----------------------------------------------------------------\n",
        "def estimate_time(device, batch_size, epochs, model, total_samples, input_size=(3, 224, 224)):\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation globally\n",
        "        # Create a random input tensor on the CPU\n",
        "        input_tensor = torch.rand(batch_size, *input_size, device='cpu')\n",
        "\n",
        "        # Move the input tensor to the GPU\n",
        "        input_tensor_list = [input_tensor[i].to(device) for i in range(batch_size)]\n",
        "\n",
        "        # Move the model to the GPU only for this estimation\n",
        "        model.to(device)\n",
        "        # Measure the time taken for a forward pass\n",
        "        start_time = time.time()\n",
        "        model(input_tensor_list)  # Pass a list of tensors to the model\n",
        "        if device==\"cuda\":\n",
        "            torch.cuda.synchronize()  # Ensure accurate timing\n",
        "        end_time = time.time()\n",
        "\n",
        "        inference_time = end_time - start_time\n",
        "\n",
        "        # Calculate the number of batches per epoch\n",
        "        batches_per_epoch = total_samples // batch_size\n",
        "\n",
        "        # Estimate training time (approximately twice the inference time)\n",
        "        training_time = 2 * inference_time * epochs * batches_per_epoch\n",
        "\n",
        "        # Move the model back to the CPU\n",
        "        model.to('cpu')\n",
        "\n",
        "        # Explicitly clear GPU cache\n",
        "        if device==\"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return training_time, inference_time\n",
        "\n",
        "\n",
        "\n",
        "device = get_device()\n",
        "# Here are some models we're about to experiment\n",
        "model_names = ['maskrcnn_resnet50_fpn', 'fasterrcnn_resnet50_fpn', 'fcos_resnet50_fpn', 'retinanet_resnet50_fpn', 'ssd300_vgg16', 'ssdlite320_mobilenet_v3_large']\n",
        "\n",
        "\n",
        "# Some settings for our experiments\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "total_samples = 50000  # Replace with your dataset size\n",
        "\n",
        "# Let's loop through the models and see how they fare\n",
        "for model_name in model_names:\n",
        "    model = get_model(model_name, device=device)  # Load the model onto the right device\n",
        "    model.eval()  # Set it to evaluation mode\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(\"Memory usage (GB):\", get_model_memory_usage(batch_size, model))\n",
        "    training_time, inference_time = estimate_time(device, batch_size, epochs, model, total_samples, input_size=(3, 800, 800))\n",
        "    print(\"Estimated training time: %.2f seconds\" % training_time)\n",
        "    print(\"Estimated inference time: %.2f seconds\" % inference_time)\n",
        "    print(\"_________________________________________________________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaRtIm0IgH-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}